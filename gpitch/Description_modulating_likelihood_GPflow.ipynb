{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description modulating_likelihood.py Model in GPflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook makes a detailed explanation of the modgp.py module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import GPflow\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first describe the function defined for calculating the weights and evaluation points for several multivariate Hermite-Gauss quadratures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mvhermgauss(means, covs, H, D):\n",
    "        \"\"\"\n",
    "        Return the evaluation locations, and weights for several multivariate Hermite-Gauss quadrature runs.\n",
    "        :param means: NxD\n",
    "        :param covs: NxDxD\n",
    "        :param H: Number of Gauss-Hermite evaluation points.\n",
    "        :param D: Number of input dimensions. Needs to be known at call-time.\n",
    "        :return: eval_locations (H**DxNxD), weights (H**D)\n",
    "        \"\"\"\n",
    "        # number of quadrature runs\n",
    "        N = tf.shape(means)[0]\n",
    "        \n",
    "        # get the standard evaluation points and weights\n",
    "        gh_x, gh_w = GPflow.likelihoods.hermgauss(H)\n",
    "        \n",
    "        # build \"mesh\" to evaluate the D dimensional input function h(X), X \\in R^D.\n",
    "        xn = np.array(list(itertools.product(*(gh_x,) * D)))  # H**DxD\n",
    "        \n",
    "        # compute the corresponding weights.\n",
    "        wn = np.prod(np.array(list(itertools.product(*(gh_w,) * D))), 1)  # H**D\n",
    "        \n",
    "        # get the standard deviation\n",
    "        cholXcov = tf.cholesky(covs)  # NxDxD\n",
    "        \n",
    "        # linear transformation of variable to integrate using quadrature.\n",
    "        X = 2.0 ** 0.5 * tf.batch_matmul(cholXcov, tf.tile(xn[None, :, :], (N, 1, 1)), adj_y=True) + tf.expand_dims(means, 2)  # NxDxH**D\n",
    "        \n",
    "        # transpose and reshape the \"mesh\" \n",
    "        Xr = tf.reshape(tf.transpose(X, [2, 0, 1]), (-1, D))  # H**DxNxD\n",
    "        \n",
    "        # return transformed evaluation points and scaled weights\n",
    "        return Xr, wn * np.pi ** (-D * 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the number of quadratures to evaluate from the number of rows of the mean vector. Then we calculate the $H$ weights and evaluation points for a single hermgauss quadrature.\n",
    "\n",
    "Then, we convert the array gh_x into a tuple by writing \"(gh_x,)\", then we muliply this tuple by $D$ the number of dimensions of the integral. This product ends up repeating the elements of the tuple $D$ times. This is done to give the input to the itertools.product() function that requires the vectors to make all the possible convinations of its elements (create mesh).\n",
    "Finally the itertool.product object is converted to list and finally to numpy ndarray.\n",
    "\n",
    "The procedure to calculate the weights is similar but the \"weight mesh\" is multiplied through its columns to finally get the corresponding weight for each function evaluation.\n",
    "\n",
    "Once given the mesh and weights to evaluate the function we need to transform the integration variable in order to get the starndar expression for the Gauss-Hermite quadrature. To do so we need to multiply the integration variables by $\\sqrt{2}$ times their corresponding standard deviation plus the corresponding mean.\n",
    "\n",
    "The standar deviation is computed by the choleski decomposition (the cov matrix is diagonal). This give us N DxD diagonal matrices, or an array of NxDxD.\n",
    "\n",
    "Once given the evaluation points without any transformation we need to transform them to calculate every of the $N$ quadratures. We first convert the matrix xn to a tensor of dimension $1 \\times H^D  \\times D$ by using \"xn[None, :, :]\".  That is why we replicate the tensor xn, i.e. the evaluation points vector (matrix) N times by using \"tf.tile(xn[None, :, :], (N, 1, 1))\" we multiply each set of evaluation points with the corresponding choleski decomposition \"tf.batch_matmul(cholXcov, tf.tile(xn[None, :, :], (N, 1, 1)), adj_y=True)\", \"adj_y=True\" means that the second \"tensor\" i.e. the repetitions of the evaluation points is transposed.\n",
    "\n",
    "To the previous value the corresponding means are added. In order to do so the mean tensor should have dimension NxDx1 that does the expression \"tf.expand_dims(means, 2)\", expands the tensor means to have dimension one in dimension index 2, then we get dimension(means) = NxDx1\n",
    "\n",
    "The obtained tensor is transposed $H^D \\times N \\times D$.\n",
    "\n",
    "the final tensor is reshaped to have dimension $-1 \\times D$ (the evaluation points for each dimension $D$). \n",
    "\n",
    "Finally the function returns the transformed evaluation points and the weights scaled by $\\frac{1}{\\pi^{-D/2}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we describe the likelihood Class used for the modulated GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModLik(GPflow.likelihoods.Likelihood):\n",
    "    def __init__(self):\n",
    "        GPflow.likelihoods.Likelihood.__init__(self)\n",
    "        self.noise_var = GPflow.param.Param(1.0)\n",
    "\n",
    "    def logp(self, F, Y):\n",
    "        f, g = F[:, 0], F[:, 1]\n",
    "        y = Y[:, 0]\n",
    "        sigma_g = 1./(1 + tf.exp(-g))  # squash g to be positive\n",
    "        mean = f * sigma_g\n",
    "        return GPflow.densities.gaussian(y, mean, self.noise_var).reshape(-1, 1)\n",
    "\n",
    "    def variational_expectations(self, Fmu, Fvar, Y):\n",
    "        # calculate evaluation points and weights\n",
    "        Xr, w = mvhermgauss(Fmu, tf.matrix_diag(Fvar), 10, 2)\n",
    "        \n",
    "        # reshape the weights\n",
    "        w = tf.reshape(w, [-1, 1])\n",
    "        \n",
    "        # get the evaluation points\n",
    "        f, g = Xr[:, 0], Xr[:, 1]\n",
    "        \n",
    "        # replicate the data H**D times\n",
    "        y = tf.tile(Y, [100, 1])[:, 0]\n",
    "        \n",
    "         # squash g to be positive\n",
    "        sigma_g = 1./(1 + tf.exp(-g)) \n",
    "        \n",
    "        # calculate mean\n",
    "        mean = f * sigma_g\n",
    "        \n",
    "        # evaluate the likelihood\n",
    "        evaluations = GPflow.densities.gaussian(y, mean, self.noise_var)\n",
    "        \n",
    "        # transpose the evaluations\n",
    "        evaluations = tf.transpose(tf.reshape(evaluations, tf.pack([tf.size(w), tf.shape(Fmu)[0]])))\n",
    "        \n",
    "        #return the weighted evaluations\n",
    "        return tf.matmul(evaluations, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ModLik likelihood class heritages from the parent Class \"GPflow.likelihoods.Likelihood\". We define the following methods:\n",
    "\n",
    "#### Constructor:\n",
    "The constructor of the parten Class is called first. Then the variance noise is defined\n",
    "\n",
    "#### logp:\n",
    "This method calculates and returns the value of the likelihood of each observation in the vector Y. The inputs are the latent functions, that is the matrix F, and the observed vector Y.\n",
    "\n",
    "#### variational_expectations:\n",
    "In this method the $N$ expectations are calculated. First, given the mean vectors and variance vectors for the variational distributions over f and g, the evaluation points and the corresponding weights are calculated. The variance vector is transform into a diagonal matrix using \"tf.matrix_diag(Fvar)\", 10 evaluation points per dimension and 2 dimensions, that is $10 \\times 10 = 100$ evaluation points.\n",
    "\n",
    "The weights are reshaped in a column vector, and the evaluation points are extracted.\n",
    "\n",
    "Then the observed data is replicated by the total number of evaluation points by dimension, that is $H^D$, and we get \"y\" with all the elements organized in one single list. This is necesary to evaluate our log_lik function at each quadrature evaluation point.\n",
    "\n",
    "After that the function evaluations are reshaped and transposed (again) and finally they are multiplied by the corresponding weight.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
