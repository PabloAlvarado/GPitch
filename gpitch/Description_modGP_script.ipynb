{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description modgp.py Model in GPflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook makes a detailed explanation of the modgp.py module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import GPflow\n",
    "from GPflow import settings\n",
    "from GPflow.tf_wraps import eye\n",
    "import tensorflow as tf\n",
    "from modulating_likelihood import ModLik # likelihood for the modulated GP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModGP(GPflow.model.Model):\n",
    "    def __init__(self, X, Y, kern1, kern2, Z):    \n",
    "        #call parent class constructor\n",
    "        GPflow.model.Model.__init__(self)\n",
    "        \n",
    "        # set atributes\n",
    "        self.X, self.Y, self.kern1, self.kern2 = X, Y, kern1, kern2\n",
    "        self.likelihood = ModLik()\n",
    "        self.Z = Z\n",
    "        self.num_inducing = Z.shape[0]\n",
    "        self.num_data = X.shape[0]\n",
    "        \n",
    "        # initialize variational mean\n",
    "        self.q_mu1 = GPflow.param.Param(np.zeros((self.Z.shape[0], 1)))\n",
    "        self.q_mu2 = GPflow.param.Param(np.zeros((self.Z.shape[0], 1)))\n",
    "        \n",
    "        # initialize variational covariance matrices\n",
    "        q_sqrt = np.array([np.eye(self.num_inducing) for _ in range(1)]).swapaxes(0, 2) \n",
    "        self.q_sqrt1 = GPflow.param.Param(q_sqrt.copy())\n",
    "        self.q_sqrt2 = GPflow.param.Param(q_sqrt.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name of our model class is ModGP, this class heritages atributes and methods from the general class GPflow.model.Model.\n",
    "\n",
    "We first define the constructor where the data, kernels and inducing points are inputs. The ModGP constructor calls then the constructor of GPflow.model.Model.\n",
    "\n",
    "Atributes are defined: input X and output data Y, kernels used, inducing variables Z, kind of likelihood, number of inducing variables and number of data observations.\n",
    "\n",
    "Then the variational parameters ared defined and initialized. that is, the mean and variance of each variational distribution. This parameters are instantiations of the class Params, the mean is initialized with a vector of $M$ zeros, where $M$ is the number of induing points.\n",
    "\n",
    "The variational square root of covariance matrices are defined by first generating a list of identity matrices by the code \"[np.eye(self.num_inducing) for _ in range(1)]\", in this case only one matrix is generated, but when passing this list trough np.array(), the array we get have dimension 1xMxM, and this array is modified to have dimension MxMx1 (.swapaxes(0, 2)). I think this shape is required for running correctly the code in GPflow when calling \"GPflow.conditionals.conditional\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we describe the second method in the class ModGP\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def build_prior_KL(self):\n",
    "        # get cov matrix prior\n",
    "        K1 = self.kern1.K(self.Z) + eye(self.num_inducing) * settings.numerics.jitter_level\n",
    "        K2 = self.kern2.K(self.Z) + eye(self.num_inducing) * settings.numerics.jitter_level\n",
    "        \n",
    "        # calculate KL div\n",
    "        KL1 = GPflow.kullback_leiblers.gauss_kl(self.q_mu1, self.q_sqrt1, K1)\n",
    "        KL2 = GPflow.kullback_leiblers.gauss_kl(self.q_mu2, self.q_sqrt2, K2)\n",
    "        return KL1 + KL2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method \"build_prior_KL\" calculates the KL divergence between the prior and the variational posterior over the inducing variables.  the cov matrix for each prior (over u_f andu_g) are altered with a little noise variance using the settings.numerics.jitter_level.\n",
    "After calculating the cov matrices ther KL divergence between each prior and variational approximation  is calculated. The prior is assumed to have zero mean function, therefore only the cov matrix in necessary, for the variational distribution both its mean vector and cov matrix are required.\n",
    "\n",
    "As shown in the report the KL divergence is equal to sum the KL divergence between each prior and approximation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we describe the \"build_likelihood\" method\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def build_likelihood(self):\n",
    "            # Get prior KL.\n",
    "            KL = self.build_prior_KL()\n",
    "\n",
    "            # Get conditionals\n",
    "            fmean1, fvar1 = GPflow.conditionals.conditional(self.X, self.Z, self.kern1, self.q_mu1,\n",
    "                                                            q_sqrt=self.q_sqrt1, full_cov=False, whiten=False)\n",
    "\n",
    "            fmean2, fvar2 = GPflow.conditionals.conditional(self.X, self.Z, self.kern2, self.q_mu2,\n",
    "                                                            q_sqrt=self.q_sqrt2, full_cov=False, whiten=False)\n",
    "\n",
    "            fmean, fvar = tf.concat(1, [fmean1, fmean2]), tf.concat(1, [fvar1, fvar2])\n",
    "\n",
    "            # Get variational expectations.\n",
    "            var_exp = self.likelihood.variational_expectations(fmean, fvar, self.Y)\n",
    "\n",
    "            # re-scale for minibatch size\n",
    "            scale = tf.cast(self.num_data, settings.dtypes.float_type) /\\\n",
    "                    tf.cast(tf.shape(self.X)[0], settings.dtypes.float_type)\n",
    "\n",
    "            return tf.reduce_sum(var_exp) * scale - KL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this method, the initial calculation of the KL is computed, then the mean and variance of the conditional distributions $q(\\textbf{f})$ and $q(\\textbf{g})$ are computed, i.e. $q(\\textbf{f}) = \\int p(\\textbf{f}|\\textbf{u}_f)q(\\textbf{u}_f) \\ \\text{d}\\textbf{u}_f$. This is done through the function \"GPflow.conditionals.conditional\".\n",
    "\n",
    "Then the means and variances for each variational distribution are concatenated.\n",
    "\n",
    "Given the mean and variance of the necesary distributions to compute the $N$ variational expectations, then we can call the function \"variational_expectations\" defined in our own likelihood Class. If I am not wrong this give us the N expectations.\n",
    "\n",
    "If a minibatch is used, then the necessary scale is computed (those lines of code need need to be analyzed properly).\n",
    "\n",
    "Finally, the sum of the $N$ variational expectations (scaled) minus the KL divergence are given back. This is the value of the Evidence Lower Bound (ELBO) !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we describe the last section for making predictions over the latent functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    @GPflow.param.AutoFlow((tf.float64, [None, None]))\n",
    "    def predict_f(self, Xnew):\n",
    "        return GPflow.conditionals.conditional(Xnew, self.Z, self.kern1, self.q_mu1,\n",
    "                                               q_sqrt=self.q_sqrt1, \n",
    "                                               full_cov=False, whiten=False)\n",
    "    \n",
    "    @GPflow.param.AutoFlow((tf.float64, [None, None]))\n",
    "    def predict_g(self, Xnew):\n",
    "        return GPflow.conditionals.conditional(Xnew, self.Z, self.kern2, self.q_mu2,\n",
    "                                               q_sqrt=self.q_sqrt2, \n",
    "                                               full_cov=False, whiten=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we make use of decorators (@); a convenient way to alter functions and methods in python.\n",
    "We define the functions for getting the prediction of each latent function, by taking the conditional given the variational posteriors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
